{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20398c3",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Classification Problem\n",
    "\n",
    "This document explores different alternatives for handling the dataset.  \n",
    "I chose to approach the problem as a classification task rather than a regression one, although the latter should not be entirely ruled out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40151b51",
   "metadata": {},
   "source": [
    "### Some Important Considerations\n",
    "\n",
    "- There are two datasets available: one for white wine and one for red wine.  \n",
    "  Both datasets share the same features, but differ significantly in the number of samples:  \n",
    "  the red wine dataset contains approximately 1,600 rows, while the white wine dataset has around 4,900 rows—roughly three times as many.  \n",
    "  This presents a key issue: deciding whether to merge the two datasets into one by adding a feature to indicate wine type, or to train two separate models for each dataset.\n",
    "\n",
    "  - If we choose to merge the datasets, we must deal with class imbalance. One solution is to perform undersampling on the white wine dataset, resulting in a merged dataset with ~3,200 rows (relatively few). Alternatively, we can apply oversampling to the red wine dataset, obtaining a dataset with approximately 10,000 rows.\n",
    "\n",
    "  - If we choose to work with the datasets separately, the resulting models might be more accurate, but we would have to manage two distinct models and two separate pipelines. Additionally, if the datasets are very similar, the resulting models might also be very similar, making it potentially unnecessary to maintain two separate ones.\n",
    "\n",
    "The most reasonable approach is to try both strategies and compare their performance using both balanced accuracy and standard accuracy (with respect to balanced and imbalanced datasets), tested on basic classification models such as Decision Trees, Support Vector Machines, and Neural Networks—without applying any preprocessing or hyperparameter tuning.  \n",
    "This will allow us to determine which approach works best for our specific case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23016c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "red_wine_data = pd.read_csv('../winequality-red.csv', sep=';')\n",
    "white_wine_data = pd.read_csv('../winequality-white.csv', sep=';')\n",
    "\n",
    "print(red_wine_data.shape)\n",
    "print(white_wine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "288fde7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "6492     11.2        6     1  \n",
       "6493      9.6        5     1  \n",
       "6494      9.4        6     1  \n",
       "6495     12.8        7     1  \n",
       "6496     11.8        6     1  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the two DataFrames\n",
    "wine_type = {'red': 0, 'white': 1}\n",
    "wine_data = pd.concat([red_wine_data, white_wine_data], ignore_index=True)\n",
    "wine_data['type'] = [wine_type['red']] * len(red_wine_data) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba1c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.3638\n",
      "SVC Balanced Accuracy: 0.2258\n",
      "MLP Balanced Accuracy: 0.2753\n",
      "Decision Tree Accuracy: 0.6115\n",
      "SVC Accuracy: 0.5608\n",
      "MLP Accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "# First method: Concatenated dataset with 3 Decision Tree, SVC and Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "X = wine_data.drop(columns=['quality'])\n",
    "y = wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf457d2",
   "metadata": {},
   "source": [
    "### Results - concatenated dataset\n",
    "| Model                | Balanced Accuracy | Accuracy |\n",
    "|------------------------|-------------------|----------|\n",
    "| Decision Tree          | 0.3638            | 0.6115   |\n",
    "| SVC                    | 0.2258            | 0.5608   |\n",
    "| MLP                    | 0.2753            | 0.5677   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186608b7",
   "metadata": {},
   "source": [
    "The results obtained so far are relatively low, but this is not a major issue, since we have not applied any preprocessing or performed any hyperparameter tuning.  \n",
    "Additionally, the models were trained on an imbalanced dataset, so low performance is to be expected.\n",
    "\n",
    "We will now evaluate the case where the same models are trained on the two datasets separately.  \n",
    "By computing the average performance across both, we can assess whether this strategy leads to any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6332da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "Decision Tree: 0.5864\n",
      "SVC: 0.5822\n",
      "MLP: 0.5935\n",
      "Decision Tree balanced: 0.3618\n",
      "SVC balanced: 0.2745\n",
      "MLP balanced: 0.3356\n",
      "\n",
      "Results for Red Wine Dataset:\n",
      "Decision Tree: 0.5625\n",
      "SVC: 0.6031\n",
      "MLP: 0.6156\n",
      "Decision Tree balanced: 0.2858\n",
      "SVC balanced: 0.2700\n",
      "MLP balanced: 0.2912\n",
      "\n",
      "Results for White Wine Dataset:\n",
      "Decision Tree: 0.6102\n",
      "SVC: 0.5612\n",
      "MLP: 0.5714\n",
      "Decision Tree balanced: 0.4379\n",
      "SVC balanced: 0.2791\n",
      "MLP balanced: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "# Second method: Separate datasets with Decision Tree, SVC and Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# red wine dataset\n",
    "X_red = red_wine_data.drop(columns=['quality'])\n",
    "y_red = red_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_red = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy,\n",
    "    'Decision Tree balanced': dt_balanced_accuracy,\n",
    "    'SVC balanced': svc_balanced_accuracy,\n",
    "    'MLP balanced': mlp_balanced_accuracy\n",
    "}\n",
    "\n",
    "# white wine dataset\n",
    "X_white = white_wine_data.drop(columns=['quality'])\n",
    "y_white = white_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_white = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy,\n",
    "    'Decision Tree balanced': dt_balanced_accuracy,\n",
    "    'SVC balanced': svc_balanced_accuracy,\n",
    "    'MLP balanced': mlp_balanced_accuracy\n",
    "}\n",
    "\n",
    "# Now I made the avarege of the accuracies for each classifier\n",
    "average_results = {\n",
    "    'Decision Tree': (results_red['Decision Tree'] + results_white['Decision Tree']) / 2,\n",
    "    'SVC': (results_red['SVC'] + results_white['SVC']) / 2,\n",
    "    'MLP': (results_red['MLP'] + results_white['MLP']) / 2,\n",
    "    'Decision Tree balanced': (results_red['Decision Tree balanced'] + results_white['Decision Tree balanced']) / 2,\n",
    "    'SVC balanced': (results_red['SVC balanced'] + results_white['SVC balanced']) / 2,\n",
    "    'MLP balanced': (results_red['MLP balanced'] + results_white['MLP balanced']) / 2\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for classifier, accuracy in average_results.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "# Print also single datasets results\n",
    "print(\"\\nResults for Red Wine Dataset:\")\n",
    "for classifier, accuracy in results_red.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nResults for White Wine Dataset:\")\n",
    "for classifier, accuracy in results_white.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e7f8",
   "metadata": {},
   "source": [
    "\n",
    "### Results - separated datasets\n",
    "\n",
    "| Model        | Average Balanced Accuracy | Average Accuracy | Red Balanced Accuracy | Red Accuracy | White Balanced Accuracy | White Accuracy |\n",
    "| -------------- | ------------------------ | ---------------- | --------------------- | ------------ | ---------------------- | -------------- |\n",
    "| Decision Tree  | 0.3618                   | 0.5864           | 0.2858                | 0.5625       | 0.4379                 | 0.6102         |\n",
    "| SVC            | 0.2745                   | 0.5822           | 0.2700                | 0.6031       | 0.2791                 | 0.5612         |\n",
    "| MLP            | 0.3356                   | 0.5935           | 0.2912                | 0.6156       | 0.3800                 | 0.5714         |\n",
    "\n",
    "### Results - concatenated datasets\n",
    "| Model                | Balanced Accuracy | Accuracy |\n",
    "|------------------------|-------------------|----------|\n",
    "| Decision Tree          | 0.3638            | 0.6115   |\n",
    "| SVC                    | 0.2258            | 0.5608   |\n",
    "| MLP                    | 0.2753            | 0.5677   |\n",
    "\n",
    "Without any kind of preprocessing or fine-tuning it is clear that working with an unbalanced dataset produces much worse performances than working with the single datasets (red and white wine). Before throwing this method away, I would still like to try to balance the rows of the datasets with undersampling and oversampling so as to have approximately the same number of rows for red and white wine.\n",
    "\n",
    "In general, the average balanced accuracy of the separate datasets has performances comparable to the balanced accuracy of the concatenated dataset, while the accuracy of the Decision Tree in the concatenated dataset surpasses all the others. In fact, an interesting behavior to note is that the Decision Tree has a lower average accuracy than the other models, but in general it performs better on the white wine dataset while there is an opposite behavior in the red wine dataset, where the SVC and the MLP perform better than the Decision Tree. This information could be reused when we need to choose the right model in case we choose to follow the method of training parallel models for separate datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06615e",
   "metadata": {},
   "source": [
    "In the following sections we will oversample the red wine dataset and undersample the white wine dataset.\n",
    "- *Oversampling*: In order to avoid overfitting with normal resampling where rows are extracted and concatenated to the original dataset creating duplicates, we decided to use SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled shape: (4086, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.200000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>7.460685</td>\n",
       "      <td>0.358786</td>\n",
       "      <td>0.319419</td>\n",
       "      <td>2.018466</td>\n",
       "      <td>0.074485</td>\n",
       "      <td>16.757260</td>\n",
       "      <td>25.577810</td>\n",
       "      <td>0.994567</td>\n",
       "      <td>3.253351</td>\n",
       "      <td>0.719419</td>\n",
       "      <td>11.569918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>8.293899</td>\n",
       "      <td>0.365820</td>\n",
       "      <td>0.393055</td>\n",
       "      <td>2.040515</td>\n",
       "      <td>0.059241</td>\n",
       "      <td>13.176834</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.995526</td>\n",
       "      <td>3.159099</td>\n",
       "      <td>0.772154</td>\n",
       "      <td>10.996139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>7.729226</td>\n",
       "      <td>0.478521</td>\n",
       "      <td>0.326338</td>\n",
       "      <td>2.260916</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>11.073933</td>\n",
       "      <td>19.390837</td>\n",
       "      <td>0.992978</td>\n",
       "      <td>3.213662</td>\n",
       "      <td>0.713169</td>\n",
       "      <td>12.519368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>8.128720</td>\n",
       "      <td>0.523680</td>\n",
       "      <td>0.157238</td>\n",
       "      <td>2.240233</td>\n",
       "      <td>0.067690</td>\n",
       "      <td>35.195346</td>\n",
       "      <td>49.333130</td>\n",
       "      <td>0.994221</td>\n",
       "      <td>3.388279</td>\n",
       "      <td>0.723564</td>\n",
       "      <td>12.565524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>7.581506</td>\n",
       "      <td>0.368579</td>\n",
       "      <td>0.402937</td>\n",
       "      <td>3.361130</td>\n",
       "      <td>0.073488</td>\n",
       "      <td>16.725175</td>\n",
       "      <td>43.635290</td>\n",
       "      <td>0.996437</td>\n",
       "      <td>3.369906</td>\n",
       "      <td>0.854692</td>\n",
       "      <td>12.959247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "1          7.800000          0.880000     0.000000        2.600000   0.098000   \n",
       "2          7.800000          0.760000     0.040000        2.300000   0.092000   \n",
       "3         11.200000          0.280000     0.560000        1.900000   0.075000   \n",
       "4          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4081       7.460685          0.358786     0.319419        2.018466   0.074485   \n",
       "4082       8.293899          0.365820     0.393055        2.040515   0.059241   \n",
       "4083       7.729226          0.478521     0.326338        2.260916   0.075317   \n",
       "4084       8.128720          0.523680     0.157238        2.240233   0.067690   \n",
       "4085       7.581506          0.368579     0.402937        3.361130   0.073488   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               11.000000             34.000000  0.997800  3.510000   \n",
       "1               25.000000             67.000000  0.996800  3.200000   \n",
       "2               15.000000             54.000000  0.997000  3.260000   \n",
       "3               17.000000             60.000000  0.998000  3.160000   \n",
       "4               11.000000             34.000000  0.997800  3.510000   \n",
       "...                   ...                   ...       ...       ...   \n",
       "4081            16.757260             25.577810  0.994567  3.253351   \n",
       "4082            13.176834             29.000000  0.995526  3.159099   \n",
       "4083            11.073933             19.390837  0.992978  3.213662   \n",
       "4084            35.195346             49.333130  0.994221  3.388279   \n",
       "4085            16.725175             43.635290  0.996437  3.369906   \n",
       "\n",
       "      sulphates    alcohol  \n",
       "0      0.560000   9.400000  \n",
       "1      0.680000   9.800000  \n",
       "2      0.650000   9.800000  \n",
       "3      0.580000   9.800000  \n",
       "4      0.560000   9.400000  \n",
       "...         ...        ...  \n",
       "4081   0.719419  11.569918  \n",
       "4082   0.772154  10.996139  \n",
       "4083   0.713169  12.519368  \n",
       "4084   0.723564  12.565524  \n",
       "4085   0.854692  12.959247  \n",
       "\n",
       "[4086 rows x 11 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try oversampling the red wine by bringing the dataset\n",
    "# to have the same number of rows as the white one using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Fit and apply SMOTE to the red wine dataset\n",
    "X_red_resampled, y_red_resampled = smote.fit_resample(X_red, y_red)\n",
    "print(f\"Resampled shape: {X_red_resampled.shape}\")\n",
    "\n",
    "X_red_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da12af",
   "metadata": {},
   "source": [
    "Now that the red wine dataset has a comparable number of rows to the white wine dataset, I merge the resampled dataset with the white one and re-run the balanced accuracy test of the 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "white    4898\n",
      "red      4086\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8982</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8984 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "8979            6.2              0.21         0.29             1.6      0.039   \n",
       "8980            6.6              0.32         0.36             8.0      0.047   \n",
       "8981            6.5              0.24         0.19             1.2      0.041   \n",
       "8982            5.5              0.29         0.30             1.1      0.022   \n",
       "8983            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "8979                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "8980                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "8981                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "8982                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "8983                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "8979     11.2        6     1  \n",
       "8980      9.6        5     1  \n",
       "8981      9.4        6     1  \n",
       "8982     12.8        7     1  \n",
       "8983     11.8        6     1  \n",
       "\n",
       "[8984 rows x 13 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# I first recreate the red wine dataset with X and y resampled\n",
    "red_wine_resampled = pd.concat([pd.DataFrame(X_red_resampled, columns=X_red.columns), \n",
    "                                 pd.Series(y_red_resampled, name='quality')], axis=1)\n",
    "\n",
    "wine_data_resampled = pd.concat([red_wine_resampled, white_wine_data], ignore_index=True)\n",
    "wine_data_resampled['type'] = [wine_type['red']] * len(red_wine_resampled) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "print(wine_data_resampled['type'].map({v: k for k, v in wine_type.items()}).value_counts())\n",
    "\n",
    "wine_data_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801c726",
   "metadata": {},
   "source": [
    "In the new concatenated dataset there are 4898 white wine samples and 4086 red wine samples, so I would say that the dataset is quite balanced, with a ratio of 1.2:1 between white wine and red wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60c17b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.6452\n",
      "SVC Balanced Accuracy: 0.5700\n",
      "MLP Balanced Accuracy: 0.6009\n",
      "Decision Tree Accuracy: 0.6956\n",
      "SVC Accuracy: 0.6194\n",
      "MLP Accuracy: 0.6450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = wine_data_resampled.drop(columns=['quality'])\n",
    "y = wine_data_resampled['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6337f4b",
   "metadata": {},
   "source": [
    "# Results on artificial balanced dataset - oversampling\n",
    "| Model | Balanced Accuracy - SMOTE | Accuracy - SMOTE |\n",
    "|---------|-------------------|-------------------|\n",
    "| Decision Tree | 0.6452 | 0.6956 |\n",
    "| SVC | 0.5700 | 0.6194 |\n",
    "| MLP | 0.6009 | 0.6450 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd83b",
   "metadata": {},
   "source": [
    "The obtained results are much better than those obtained with the unbalanced dataset (w.r.t. number of red and white wine samples), both for balanced accuracy (which in this case is less relevant since the dataset has been balanced) and accuracy. In particular, the Decision Tree obtained the best performance, followed by the MLP and the SVC. This suggests that the Decision Tree could be the most suitable model for this balanced dataset. Furthermore, these results are much better than those obtained with the separate datasets, which suggests that merging the datasets could be a good strategy to improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce687b",
   "metadata": {},
   "source": [
    "All that remains is to undersample the white wine dataset, in order to have a balanced dataset with a number of rows comparable to that of the red wine. In this case, the white wine dataset will be reduced to 1600 rows, while the red wine dataset will remain unchanged. Furthermore, since many of the values ​​are distributed around the `quality` class of 5 or 6, we decided to undersample trying to maintain the proportions of the classes, in order not to lose important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e61bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.448755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.297468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.179665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         proportion\n",
       "quality            \n",
       "6          0.448755\n",
       "5          0.297468\n",
       "7          0.179665\n",
       "8          0.035729\n",
       "4          0.033279\n",
       "3          0.004083\n",
       "9          0.001021"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_target = len(red_wine_data)\n",
    "\n",
    "white_dist = white_wine_data['quality'].value_counts(normalize=True)\n",
    "\n",
    "pd.DataFrame(white_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f4d18",
   "metadata": {},
   "source": [
    "As predicted, many of the quality values ​​are represented by the $5$ and $6$ classes. I want to maintain the class proportions even in the undersampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3005408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>38.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.024</td>\n",
       "      <td>20.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.99208</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.119</td>\n",
       "      <td>33.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99210</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>20.30</td>\n",
       "      <td>0.037</td>\n",
       "      <td>45.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.99727</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.046</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.99775</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>15.55</td>\n",
       "      <td>0.057</td>\n",
       "      <td>35.5</td>\n",
       "      <td>366.5</td>\n",
       "      <td>1.00010</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>10.3</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.037</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.99390</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.28</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>146.5</td>\n",
       "      <td>307.5</td>\n",
       "      <td>0.99240</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.021</td>\n",
       "      <td>24.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.98965</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.61</td>\n",
       "      <td>12.4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.032</td>\n",
       "      <td>31.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99030</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>12.9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.2              0.15         0.46            1.60      0.039   \n",
       "1               6.9              0.31         0.32            1.20      0.024   \n",
       "2               6.0              0.28         0.34            1.60      0.119   \n",
       "3               6.8              0.30         0.26           20.30      0.037   \n",
       "4               6.2              0.30         0.26           13.40      0.046   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1595            8.6              0.55         0.35           15.55      0.057   \n",
       "1596           10.3              0.17         0.47            1.40      0.037   \n",
       "1597            7.1              0.49         0.22            2.00      0.047   \n",
       "1598            6.6              0.36         0.29            1.60      0.021   \n",
       "1599            7.1              0.26         0.49            2.20      0.032   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    38.0                 123.0  0.99300  3.38       0.51   \n",
       "1                    20.0                 166.0  0.99208  3.05       0.54   \n",
       "2                    33.0                 104.0  0.99210  3.19       0.38   \n",
       "3                    45.0                 150.0  0.99727  3.04       0.38   \n",
       "4                    57.0                 206.0  0.99775  3.17       0.43   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1595                 35.5                 366.5  1.00010  3.04       0.63   \n",
       "1596                  5.0                  33.0  0.99390  2.89       0.28   \n",
       "1597                146.5                 307.5  0.99240  3.24       0.37   \n",
       "1598                 24.0                  85.0  0.98965  3.41       0.61   \n",
       "1599                 31.0                 113.0  0.99030  3.37       0.42   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.7        6  \n",
       "1         9.8        6  \n",
       "2        10.2        6  \n",
       "3        12.3        6  \n",
       "4         9.5        6  \n",
       "...       ...      ...  \n",
       "1595     11.0        3  \n",
       "1596      9.6        3  \n",
       "1597     11.0        3  \n",
       "1598     12.4        9  \n",
       "1599     12.9        9  \n",
       "\n",
       "[1600 rows x 12 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ottengo il numero di campioni desiderati per ogni classe\n",
    "n_per_class = (white_dist * n_target).round().astype(int)\n",
    "\n",
    "white_wine_undersampled = pd.DataFrame()\n",
    "for label, n_samples in n_per_class.items():\n",
    "    subset = white_wine_data[white_wine_data['quality'] == label]\n",
    "    sampled_subset = subset.sample(n=n_samples, random_state=42)\n",
    "    white_wine_undersampled = pd.concat([white_wine_undersampled, sampled_subset], ignore_index=True)\n",
    "\n",
    "white_wine_undersampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8bf4a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.3704\n",
      "SVC Balanced Accuracy: 0.2695\n",
      "MLP Balanced Accuracy: 0.2803\n",
      "Decision Tree Accuracy: 0.5391\n",
      "SVC Accuracy: 0.5875\n",
      "MLP Accuracy: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now I create the final dataset with the undersampled white wine\n",
    "wine_data_undersampled = pd.concat([red_wine_data, white_wine_undersampled], ignore_index=True)\n",
    "wine_data_undersampled['type'] = [wine_type['red']] * len(red_wine_data) + [wine_type['white']] * len(white_wine_undersampled)\n",
    "\n",
    "X = wine_data_undersampled.drop(columns=['quality'])\n",
    "y = wine_data_undersampled['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff44bc4",
   "metadata": {},
   "source": [
    "# Results on artificial balanced dataset  - undersampling\n",
    "| Model | Balanced Accuracy | Accuracy |\n",
    "|---------|-------------------|-------------------|\n",
    "| Decision Tree | 0.3704 | 0.5391 |\n",
    "| SVC | 0.2695 | 0.5875 |\n",
    "| MLP | 0.2803 | 0.5609 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5c2e7",
   "metadata": {},
   "source": [
    "The performance of balanced accuracy and accuracy is very low compared to that obtained with oversampling, which suggests that undersampling may not be the best strategy for this dataset. In particular, Decision Tree achieved the best performance using balanced accuracy while SVC achieved the best performance using accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91008d",
   "metadata": {},
   "source": [
    "Here is a unique table comparing the results obtained with different balancing methods and classification models:\n",
    "\n",
    "| Model        | Balanced Accuracy (Undersampling) | Accuracy (Undersampling) | Balanced Accuracy (Oversampling/SMOTE) | Accuracy (Oversampling/SMOTE) |\n",
    "| -------------- | --------------------------------- | ------------------------ | -------------------------------------- | ----------------------------- |\n",
    "| Decision Tree  | 0.3704                            | 0.5391                   | 0.6452                                 | 0.6956                        |\n",
    "| SVC            | 0.2695                            | 0.5875                   | 0.5700                                 | 0.6194                        |\n",
    "| MLP            | 0.2803                            | 0.5609                   | 0.6009                                 | 0.6450                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bbe83",
   "metadata": {},
   "source": [
    "Oversampling with SMOTE produced significantly better results than undersampling, both in terms of balanced accuracy and accuracy. In particular, Decision Tree achieved the best performance in both cases, followed by MLP and SVC. This suggests that oversampling could be the best strategy for this dataset, as it allows to keep a larger number of samples and therefore to preserve more useful information for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d39e2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "At this stage, we are left with two main options:  \n",
    "- Apply SMOTE oversampling to the red wine dataset and then merge it with the white wine dataset, resulting in a balanced dataset with approximately 9,000 rows.  \n",
    "- Train the models separately on the two datasets and compute the average balanced accuracy and average accuracy across both.\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "| Model          | Average Balanced Accuracy | Average Accuracy | Balanced Accuracy - SMOTE | Accuracy - SMOTE |\n",
    "| -------------- | :------------------------: | :--------------: | :------------------------: | :--------------: |\n",
    "| Decision Tree  |           0.3618           |      0.5864       |           0.6452           |      0.6956       |\n",
    "| SVC            |           0.2745           |      0.5822       |           0.5700           |      0.6194       |\n",
    "| MLP            |           0.3356           |      0.5935       |           0.6009           |      0.6450       |\n",
    "\n",
    "### Explored Options:\n",
    "- Concatenated dataset  \n",
    "- Concatenated dataset with oversampling (SMOTE)  \n",
    "- Concatenated dataset with undersampling  \n",
    "- Separate datasets  \n",
    "\n",
    "### Selected Option:\n",
    "- Concatenated dataset with oversampling (SMOTE)\n",
    "\n",
    "In conclusion, the SMOTE-based oversampling strategy led to better results compared to all other alternatives considered.  \n",
    "However, it's important to note that the overall performance is still far from optimal and could be further improved through proper feature selection, hyperparameter tuning, and more advanced preprocessing techniques.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rossi-manno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
