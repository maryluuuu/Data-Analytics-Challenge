{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40151b51",
   "metadata": {},
   "source": [
    "### Alcune considerazioni importanti da fare\n",
    "\n",
    "### Alcune considerazioni importanti da fare\n",
    "\n",
    "\n",
    "- Esistono due dataset, uno per il vino bianco e uno per il vino rosso, il numero di features è lo stesso mentre cambiano il numero di sample dei singoli dataset: per il vino rosso abbiamo circa 1600 righe mentre per il vino bianco abbiamo 4900 righe, cioè 3 volte tanto. Questo porta in luce un problema, cioè scegliere se fondere i due dataset in uno solo aggiungengo una feature per indicare il tipo di vino oppure sviluppare parallelamente due modelli per i singoli dataset.\n",
    "  - Se scegliessimo di fondere i due dataset, dovremmo gestirne uno non bilanciato, alternativamente si può pensare di fare undersampling sul dataset dei vini bianchi portando alla creazione di un unico dataset con 3200 righe (relativamente poche) o fare oversampling sul dataset dei vini rossi avendo un dataset di approsimativamente 10000 righe.\n",
    "  - Se scegliessimo di lavorare con i due dataset separati i modelli potrebbero essere più performanti, ma dovremmo gestire due modelli distinti e quindi due pipeline di lavoro distinte. Inoltre, se i due dataset sono molto simili, potremmo avere dei modelli che si sovrappongono molto e quindi non sarebbe necessario sviluppare due modelli distinti.\n",
    "\n",
    "L'approccio che trovo più ragionevole è quello di provare entrambi i metodi e di confrontare la balanced accuracy e l'accuracy (w.r.t. dataset bilanciato e dataset sbilanciati) su modelli di classificazione quali Decision Tree, Support Vector Machine e Neural Network (senza alcun tipo di preprocessing nè scelta degli iperparametri). In questo modo potremo valutare quale approccio funziona meglio per il nostro caso specifico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23016c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "red_wine_data = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_wine_data = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "print(red_wine_data.shape)\n",
    "print(white_wine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288fde7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "6492     11.2        6     1  \n",
       "6493      9.6        5     1  \n",
       "6494      9.4        6     1  \n",
       "6495     12.8        7     1  \n",
       "6496     11.8        6     1  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the two DataFrames\n",
    "wine_type = {'red': 0, 'white': 1}\n",
    "wine_data = pd.concat([red_wine_data, white_wine_data], ignore_index=True)\n",
    "wine_data['type'] = [wine_type['red']] * len(red_wine_data) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dba1c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.3638\n",
      "SVC Balanced Accuracy: 0.2258\n",
      "MLP Balanced Accuracy: 0.2753\n"
     ]
    }
   ],
   "source": [
    "# Primo metodo: dataset concatenato con i 3 Decision Tree, SVC e Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "X = wine_data.drop(columns=['quality'])\n",
    "y = wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186608b7",
   "metadata": {},
   "source": [
    "I risultati ottenuto sono abbastanza bassi, ma non è un problema, in quanto non abbiamo fatto alcun tipo di preprocessing e non abbiamo scelto gli iperparametri. Inoltre, i modelli sono stati addestrati su un dataset sbilanciato, quindi è normale che le performance siano basse. Ora verifichiamo il caso in cui alleno gl stessi modelli su i due dataset separati e vediamo se le performance migliorano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6332da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "Decision Tree: 0.5864\n",
      "SVC: 0.5822\n",
      "MLP: 0.5935\n",
      "\n",
      "Results for Red Wine Dataset:\n",
      "Decision Tree: 0.5625\n",
      "SVC: 0.6031\n",
      "MLP: 0.6156\n",
      "\n",
      "Results for White Wine Dataset:\n",
      "Decision Tree: 0.6102\n",
      "SVC: 0.5612\n",
      "MLP: 0.5714\n"
     ]
    }
   ],
   "source": [
    "# Secondo metodo: datasets separati con Decision Tree, SVC e Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# Prima il dataset del vino rosso\n",
    "X_red = red_wine_data.drop(columns=['quality'])\n",
    "y_red = red_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_red = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy\n",
    "}\n",
    "\n",
    "# Ora il dataset del vino bianco\n",
    "X_white = white_wine_data.drop(columns=['quality'])\n",
    "y_white = white_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_white = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy\n",
    "}\n",
    "\n",
    "# Now I made the avarege of the accuracies for each classifier\n",
    "average_results = {\n",
    "    'Decision Tree': (results_red['Decision Tree'] + results_white['Decision Tree']) / 2,\n",
    "    'SVC': (results_red['SVC'] + results_white['SVC']) / 2,\n",
    "    'MLP': (results_red['MLP'] + results_white['MLP']) / 2\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for classifier, accuracy in average_results.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "# Voglio stampare anche i risultati dei singoli dataset\n",
    "print(\"\\nResults for Red Wine Dataset:\")\n",
    "for classifier, accuracy in results_red.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nResults for White Wine Dataset:\")\n",
    "for classifier, accuracy in results_white.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e7f8",
   "metadata": {},
   "source": [
    "\n",
    "### Risultati ottenuti\n",
    "\n",
    "| Modello | Balanced Accuracy | Average Accuracy | Red Wine | White Wine |\n",
    "|---------|-------------------|------------------|----------|------------|\n",
    "| Decision Tree | 0.3638 | 0.5864 | 0.5625 | 0.6102 |\n",
    "| SVC | 0.2258 | 0.5822 | 0.6031 | 0.5612 |\n",
    "| MLP | 0.2753 | 0.5935 | 0.6156 | 0.5714 |\n",
    "\n",
    "Senza alcun tipo di preprocessing nè di fine-tuning è chiaro che lavorare con un dataset sbilanciato produce performance molto peggiori rispetto a lavorare con i singoli dataset (red e white wine). Prima di buttare questo metodo, vorrei comunque provare a bilanciare i dataset con l'undersampling e l'oversampling. Un comportamento interessante da notare è che il Decision Tree ha una media più bassa rispetto agli altri modelli, ma in generale performa meglio sul dataset del vino bianco mentre esiste un comportamento contrario nel dataset del vino rosso, dove la SVC e la MLP performano meglio del Decision Tree. Queste informazioni potrebbero essere riutilizzati nel momento in cui dovremmo scegliere il modello giusto nel caso segliessimo di seguire il metodo di allenare modelli paralleli per i dataset separati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06615e",
   "metadata": {},
   "source": [
    "Nelle successive sezioni effettuerò l'oversampling del dataset dei vini rossi e l'undersampling dei vini bianchi.\n",
    "- *Oversampling*: al fine di evitare overfitting con il resampling normale in cui le righe vengono estratte e concatenate al dataset originale creando duplicati, ho deciso di utilizzare SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5809f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled shape: (4086, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.200000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>7.460685</td>\n",
       "      <td>0.358786</td>\n",
       "      <td>0.319419</td>\n",
       "      <td>2.018466</td>\n",
       "      <td>0.074485</td>\n",
       "      <td>16.757260</td>\n",
       "      <td>25.577810</td>\n",
       "      <td>0.994567</td>\n",
       "      <td>3.253351</td>\n",
       "      <td>0.719419</td>\n",
       "      <td>11.569918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>8.293899</td>\n",
       "      <td>0.365820</td>\n",
       "      <td>0.393055</td>\n",
       "      <td>2.040515</td>\n",
       "      <td>0.059241</td>\n",
       "      <td>13.176834</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.995526</td>\n",
       "      <td>3.159099</td>\n",
       "      <td>0.772154</td>\n",
       "      <td>10.996139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>7.729226</td>\n",
       "      <td>0.478521</td>\n",
       "      <td>0.326338</td>\n",
       "      <td>2.260916</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>11.073933</td>\n",
       "      <td>19.390837</td>\n",
       "      <td>0.992978</td>\n",
       "      <td>3.213662</td>\n",
       "      <td>0.713169</td>\n",
       "      <td>12.519368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>8.128720</td>\n",
       "      <td>0.523680</td>\n",
       "      <td>0.157238</td>\n",
       "      <td>2.240233</td>\n",
       "      <td>0.067690</td>\n",
       "      <td>35.195346</td>\n",
       "      <td>49.333130</td>\n",
       "      <td>0.994221</td>\n",
       "      <td>3.388279</td>\n",
       "      <td>0.723564</td>\n",
       "      <td>12.565524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>7.581506</td>\n",
       "      <td>0.368579</td>\n",
       "      <td>0.402937</td>\n",
       "      <td>3.361130</td>\n",
       "      <td>0.073488</td>\n",
       "      <td>16.725175</td>\n",
       "      <td>43.635290</td>\n",
       "      <td>0.996437</td>\n",
       "      <td>3.369906</td>\n",
       "      <td>0.854692</td>\n",
       "      <td>12.959247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "1          7.800000          0.880000     0.000000        2.600000   0.098000   \n",
       "2          7.800000          0.760000     0.040000        2.300000   0.092000   \n",
       "3         11.200000          0.280000     0.560000        1.900000   0.075000   \n",
       "4          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4081       7.460685          0.358786     0.319419        2.018466   0.074485   \n",
       "4082       8.293899          0.365820     0.393055        2.040515   0.059241   \n",
       "4083       7.729226          0.478521     0.326338        2.260916   0.075317   \n",
       "4084       8.128720          0.523680     0.157238        2.240233   0.067690   \n",
       "4085       7.581506          0.368579     0.402937        3.361130   0.073488   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               11.000000             34.000000  0.997800  3.510000   \n",
       "1               25.000000             67.000000  0.996800  3.200000   \n",
       "2               15.000000             54.000000  0.997000  3.260000   \n",
       "3               17.000000             60.000000  0.998000  3.160000   \n",
       "4               11.000000             34.000000  0.997800  3.510000   \n",
       "...                   ...                   ...       ...       ...   \n",
       "4081            16.757260             25.577810  0.994567  3.253351   \n",
       "4082            13.176834             29.000000  0.995526  3.159099   \n",
       "4083            11.073933             19.390837  0.992978  3.213662   \n",
       "4084            35.195346             49.333130  0.994221  3.388279   \n",
       "4085            16.725175             43.635290  0.996437  3.369906   \n",
       "\n",
       "      sulphates    alcohol  \n",
       "0      0.560000   9.400000  \n",
       "1      0.680000   9.800000  \n",
       "2      0.650000   9.800000  \n",
       "3      0.580000   9.800000  \n",
       "4      0.560000   9.400000  \n",
       "...         ...        ...  \n",
       "4081   0.719419  11.569918  \n",
       "4082   0.772154  10.996139  \n",
       "4083   0.713169  12.519368  \n",
       "4084   0.723564  12.565524  \n",
       "4085   0.854692  12.959247  \n",
       "\n",
       "[4086 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proviamo a fare oversampling sul red wine portando il dataset\n",
    "# ad avere lo stesso numero di righe di quello bianco utilizzando SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Fit and apply SMOTE to the red wine dataset\n",
    "X_red_resampled, y_red_resampled = smote.fit_resample(X_red, y_red)\n",
    "print(f\"Resampled shape: {X_red_resampled.shape}\")\n",
    "\n",
    "X_red_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da12af",
   "metadata": {},
   "source": [
    "Ora che il dataset del vino rosso possiede un numero di righe confrontabili con quello del vino bianco, unisco il dataset resampled con quello bainco e rieffettuo il test sulla balanced accuracy dei 3 modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "1    4898\n",
      "0    4086\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# Ricreo prima il dataset del vino rosso con X e y resampled\n",
    "red_wine_resampled = pd.concat([pd.DataFrame(X_red_resampled, columns=X_red.columns), \n",
    "                                 pd.Series(y_red_resampled, name='quality')], axis=1)\n",
    "\n",
    "wine_data_resampled = pd.concat([red_wine_resampled, white_wine_data], ignore_index=True)\n",
    "wine_data_resampled['type'] = [wine_type['red']] * len(red_wine_resampled) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "print(wine_data_resampled['type'].value_counts())\n",
    "\n",
    "wine_data_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801c726",
   "metadata": {},
   "source": [
    "Nel nuovo dataset concatenato ci sono 4898 sample di vino bianco e 4086 sample di vino rosso, quindi direi che il dataset è abbastanza bilanciato, con un rapporto di 1.2:1 tra il vino bianco e il vino rosso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60c17b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.6452\n",
      "SVC Balanced Accuracy: 0.5700\n",
      "MLP Balanced Accuracy: 0.6009\n",
      "Decision Tree Accuracy: 0.6956\n",
      "SVC Accuracy: 0.6194\n",
      "MLP Accuracy: 0.6450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = wine_data_resampled.drop(columns=['quality'])\n",
    "y = wine_data_resampled['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6337f4b",
   "metadata": {},
   "source": [
    "# Risultati ottenuti su dataset artificiale bilanciato\n",
    "| Modello | Balanced Accuracy - Oversampling | Accuracy - Oversampling |\n",
    "|---------|-------------------|-------------------|\n",
    "| Decision Tree | 0.6452 | 0.6956 |\n",
    "| SVC | 0.5700 | 0.6194 |\n",
    "| MLP | 0.6009 | 0.6450 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd83b",
   "metadata": {},
   "source": [
    "I risultati ottenuti sono molto migliori rispetto a quelli ottenuti con il dataset sbilanciato, sia per quanto riguarda la balanced accuracy (che in questo caso è meno rilevante dal momento che il dataset è stato bilanciato) che l'accuracy. In particolare, il Decision Tree ha ottenuto la migliore performance, seguito dalla MLP e dalla SVC. Questo suggerisce che il Decision Tree potrebbe essere il modello più adatto per questo dataset bilanciato. Inoltre, questi risultati sono molto migliori di quelli ottenuti con i dataset separati, il che suggerisce che la fusione dei dataset potrebbe essere una buona strategia per migliorare le performance del modello."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rossi-manno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
