{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40151b51",
   "metadata": {},
   "source": [
    "### Alcune considerazioni importanti da fare\n",
    "\n",
    "### Alcune considerazioni importanti da fare\n",
    "\n",
    "\n",
    "- Esistono due dataset, uno per il vino bianco e uno per il vino rosso, il numero di features è lo stesso mentre cambiano il numero di sample dei singoli dataset: per il vino rosso abbiamo circa 1600 righe mentre per il vino bianco abbiamo 4900 righe, cioè 3 volte tanto. Questo porta in luce un problema, cioè scegliere se fondere i due dataset in uno solo aggiungengo una feature per indicare il tipo di vino oppure sviluppare parallelamente due modelli per i singoli dataset.\n",
    "  - Se scegliessimo di fondere i due dataset, dovremmo gestirne uno non bilanciato, alternativamente si può pensare di fare undersampling sul dataset dei vini bianchi portando alla creazione di un unico dataset con 3200 righe (relativamente poche) o fare oversampling sul dataset dei vini rossi avendo un dataset di approsimativamente 10000 righe.\n",
    "  - Se scegliessimo di lavorare con i due dataset separati i modelli potrebbero essere più performanti, ma dovremmo gestire due modelli distinti e quindi due pipeline di lavoro distinte. Inoltre, se i due dataset sono molto simili, potremmo avere dei modelli che si sovrappongono molto e quindi non sarebbe necessario sviluppare due modelli distinti.\n",
    "\n",
    "L'approccio che trovo più ragionevole è quello di provare entrambi i metodi e di confrontare la balanced accuracy e l'accuracy (w.r.t. dataset bilanciato e dataset sbilanciati) su modelli di classificazione quali Decision Tree, Support Vector Machine e Neural Network (senza alcun tipo di preprocessing nè scelta degli iperparametri). In questo modo potremo valutare quale approccio funziona meglio per il nostro caso specifico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a23016c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n",
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "red_wine_data = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_wine_data = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "print(red_wine_data.shape)\n",
    "print(white_wine_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "288fde7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "6492            6.2              0.21         0.29             1.6      0.039   \n",
       "6493            6.6              0.32         0.36             8.0      0.047   \n",
       "6494            6.5              0.24         0.19             1.2      0.041   \n",
       "6495            5.5              0.29         0.30             1.1      0.022   \n",
       "6496            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "6492                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "6493                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "6494                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "6495                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "6496                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "6492     11.2        6     1  \n",
       "6493      9.6        5     1  \n",
       "6494      9.4        6     1  \n",
       "6495     12.8        7     1  \n",
       "6496     11.8        6     1  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the two DataFrames\n",
    "wine_type = {'red': 0, 'white': 1}\n",
    "wine_data = pd.concat([red_wine_data, white_wine_data], ignore_index=True)\n",
    "wine_data['type'] = [wine_type['red']] * len(red_wine_data) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6dba1c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.3638\n",
      "SVC Balanced Accuracy: 0.2258\n",
      "MLP Balanced Accuracy: 0.2753\n",
      "Decision Tree Accuracy: 0.6115\n",
      "SVC Accuracy: 0.5608\n",
      "MLP Accuracy: 0.5677\n"
     ]
    }
   ],
   "source": [
    "# Primo metodo: dataset concatenato con i 3 Decision Tree, SVC e Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "X = wine_data.drop(columns=['quality'])\n",
    "y = wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf457d2",
   "metadata": {},
   "source": [
    "### Risultati ottenuti - dataset concatenato\n",
    "| Modello                | Balanced Accuracy | Accuracy |\n",
    "|------------------------|-------------------|----------|\n",
    "| Decision Tree          | 0.3638            | 0.6115   |\n",
    "| SVC                    | 0.2258            | 0.5608   |\n",
    "| MLP                    | 0.2753            | 0.5677   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186608b7",
   "metadata": {},
   "source": [
    "I risultati ottenuto sono abbastanza bassi, ma non è un problema, in quanto non abbiamo fatto alcun tipo di preprocessing e non abbiamo scelto gli iperparametri. Inoltre, i modelli sono stati addestrati su un dataset sbilanciato, quindi è normale che le performance siano basse. Ora verifichiamo il caso in cui alleno gl stessi modelli su i due dataset separati facendo poi la media tra i due e vediamo se le performance migliorano.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6332da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "Decision Tree: 0.5864\n",
      "SVC: 0.5822\n",
      "MLP: 0.5935\n",
      "Decision Tree balanced: 0.3618\n",
      "SVC balanced: 0.2745\n",
      "MLP balanced: 0.3356\n",
      "\n",
      "Results for Red Wine Dataset:\n",
      "Decision Tree: 0.5625\n",
      "SVC: 0.6031\n",
      "MLP: 0.6156\n",
      "Decision Tree balanced: 0.2858\n",
      "SVC balanced: 0.2700\n",
      "MLP balanced: 0.2912\n",
      "\n",
      "Results for White Wine Dataset:\n",
      "Decision Tree: 0.6102\n",
      "SVC: 0.5612\n",
      "MLP: 0.5714\n",
      "Decision Tree balanced: 0.4379\n",
      "SVC balanced: 0.2791\n",
      "MLP balanced: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "# Secondo metodo: datasets separati con Decision Tree, SVC e Neural Network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# Prima il dataset del vino rosso\n",
    "X_red = red_wine_data.drop(columns=['quality'])\n",
    "y_red = red_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_red, y_red, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_red = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy,\n",
    "    'Decision Tree balanced': dt_balanced_accuracy,\n",
    "    'SVC balanced': svc_balanced_accuracy,\n",
    "    'MLP balanced': mlp_balanced_accuracy\n",
    "}\n",
    "\n",
    "# Ora il dataset del vino bianco\n",
    "X_white = white_wine_data.drop(columns=['quality'])\n",
    "y_white = white_wine_data['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "results_white = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'SVC': svc_accuracy,\n",
    "    'MLP': mlp_accuracy,\n",
    "    'Decision Tree balanced': dt_balanced_accuracy,\n",
    "    'SVC balanced': svc_balanced_accuracy,\n",
    "    'MLP balanced': mlp_balanced_accuracy\n",
    "}\n",
    "\n",
    "# Now I made the avarege of the accuracies for each classifier\n",
    "average_results = {\n",
    "    'Decision Tree': (results_red['Decision Tree'] + results_white['Decision Tree']) / 2,\n",
    "    'SVC': (results_red['SVC'] + results_white['SVC']) / 2,\n",
    "    'MLP': (results_red['MLP'] + results_white['MLP']) / 2,\n",
    "    'Decision Tree balanced': (results_red['Decision Tree balanced'] + results_white['Decision Tree balanced']) / 2,\n",
    "    'SVC balanced': (results_red['SVC balanced'] + results_white['SVC balanced']) / 2,\n",
    "    'MLP balanced': (results_red['MLP balanced'] + results_white['MLP balanced']) / 2\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for classifier, accuracy in average_results.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "# Voglio stampare anche i risultati dei singoli dataset\n",
    "print(\"\\nResults for Red Wine Dataset:\")\n",
    "for classifier, accuracy in results_red.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nResults for White Wine Dataset:\")\n",
    "for classifier, accuracy in results_white.items():\n",
    "    print(f\"{classifier}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7e7f8",
   "metadata": {},
   "source": [
    "\n",
    "### Risultati ottenuti - dataset separati\n",
    "\n",
    "| Modello        | Average Balanced Accuracy | Average Accuracy | Red Balanced Accuracy | Red Accuracy | White Balanced Accuracy | White Accuracy |\n",
    "| -------------- | ------------------------ | ---------------- | --------------------- | ------------ | ---------------------- | -------------- |\n",
    "| Decision Tree  | 0.3618                   | 0.5864           | 0.2858                | 0.5625       | 0.4379                 | 0.6102         |\n",
    "| SVC            | 0.2745                   | 0.5822           | 0.2700                | 0.6031       | 0.2791                 | 0.5612         |\n",
    "| MLP            | 0.3356                   | 0.5935           | 0.2912                | 0.6156       | 0.3800                 | 0.5714         |\n",
    "\n",
    "### Risultati ottenuti - dataset concatenato\n",
    "| Modello                | Balanced Accuracy | Accuracy |\n",
    "|------------------------|-------------------|----------|\n",
    "| Decision Tree          | 0.3638            | 0.6115   |\n",
    "| SVC                    | 0.2258            | 0.5608   |\n",
    "| MLP                    | 0.2753            | 0.5677   |\n",
    "\n",
    "Senza alcun tipo di preprocessing nè di fine-tuning è chiaro che lavorare con un dataset sbilanciato produce performance molto peggiori rispetto a lavorare con i singoli dataset (red e white wine). Prima di buttare questo metodo, vorrei comunque provare a bilanciare le righe dei dataset con l'undersampling e l'oversampling in modo da avere circa lo stesso numero di righe per vino rosso e bianco. \n",
    "\n",
    "In generale, l'average balanced accuracy dei dataset separati ha performance paragonabili alla balanced accuracy del dataset concatenato, mentre l'accuracy del Decision Tree nel dataset concatenato supera tutte le altre. Infatti, un comportamento interessante da notare è che il Decision Tree ha una accuracy media più bassa rispetto agli altri modelli, ma in generale performa meglio sul dataset del vino bianco mentre esiste un comportamento contrario nel dataset del vino rosso, dove la SVC e la MLP performano meglio del Decision Tree. Queste informazioni potrebbero essere riutilizzati nel momento in cui dovremmo scegliere il modello giusto nel caso segliessimo di seguire il metodo di allenare modelli paralleli per i dataset separati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca06615e",
   "metadata": {},
   "source": [
    "Nelle successive sezioni effettuerò l'oversampling del dataset dei vini rossi e l'undersampling dei vini bianchi.\n",
    "- *Oversampling*: al fine di evitare overfitting con il resampling normale in cui le righe vengono estratte e concatenate al dataset originale creando duplicati, ho deciso di utilizzare SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5809f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled shape: (4086, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>3.260000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.200000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>3.160000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>9.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>7.460685</td>\n",
       "      <td>0.358786</td>\n",
       "      <td>0.319419</td>\n",
       "      <td>2.018466</td>\n",
       "      <td>0.074485</td>\n",
       "      <td>16.757260</td>\n",
       "      <td>25.577810</td>\n",
       "      <td>0.994567</td>\n",
       "      <td>3.253351</td>\n",
       "      <td>0.719419</td>\n",
       "      <td>11.569918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>8.293899</td>\n",
       "      <td>0.365820</td>\n",
       "      <td>0.393055</td>\n",
       "      <td>2.040515</td>\n",
       "      <td>0.059241</td>\n",
       "      <td>13.176834</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.995526</td>\n",
       "      <td>3.159099</td>\n",
       "      <td>0.772154</td>\n",
       "      <td>10.996139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>7.729226</td>\n",
       "      <td>0.478521</td>\n",
       "      <td>0.326338</td>\n",
       "      <td>2.260916</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>11.073933</td>\n",
       "      <td>19.390837</td>\n",
       "      <td>0.992978</td>\n",
       "      <td>3.213662</td>\n",
       "      <td>0.713169</td>\n",
       "      <td>12.519368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>8.128720</td>\n",
       "      <td>0.523680</td>\n",
       "      <td>0.157238</td>\n",
       "      <td>2.240233</td>\n",
       "      <td>0.067690</td>\n",
       "      <td>35.195346</td>\n",
       "      <td>49.333130</td>\n",
       "      <td>0.994221</td>\n",
       "      <td>3.388279</td>\n",
       "      <td>0.723564</td>\n",
       "      <td>12.565524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>7.581506</td>\n",
       "      <td>0.368579</td>\n",
       "      <td>0.402937</td>\n",
       "      <td>3.361130</td>\n",
       "      <td>0.073488</td>\n",
       "      <td>16.725175</td>\n",
       "      <td>43.635290</td>\n",
       "      <td>0.996437</td>\n",
       "      <td>3.369906</td>\n",
       "      <td>0.854692</td>\n",
       "      <td>12.959247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "1          7.800000          0.880000     0.000000        2.600000   0.098000   \n",
       "2          7.800000          0.760000     0.040000        2.300000   0.092000   \n",
       "3         11.200000          0.280000     0.560000        1.900000   0.075000   \n",
       "4          7.400000          0.700000     0.000000        1.900000   0.076000   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4081       7.460685          0.358786     0.319419        2.018466   0.074485   \n",
       "4082       8.293899          0.365820     0.393055        2.040515   0.059241   \n",
       "4083       7.729226          0.478521     0.326338        2.260916   0.075317   \n",
       "4084       8.128720          0.523680     0.157238        2.240233   0.067690   \n",
       "4085       7.581506          0.368579     0.402937        3.361130   0.073488   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "0               11.000000             34.000000  0.997800  3.510000   \n",
       "1               25.000000             67.000000  0.996800  3.200000   \n",
       "2               15.000000             54.000000  0.997000  3.260000   \n",
       "3               17.000000             60.000000  0.998000  3.160000   \n",
       "4               11.000000             34.000000  0.997800  3.510000   \n",
       "...                   ...                   ...       ...       ...   \n",
       "4081            16.757260             25.577810  0.994567  3.253351   \n",
       "4082            13.176834             29.000000  0.995526  3.159099   \n",
       "4083            11.073933             19.390837  0.992978  3.213662   \n",
       "4084            35.195346             49.333130  0.994221  3.388279   \n",
       "4085            16.725175             43.635290  0.996437  3.369906   \n",
       "\n",
       "      sulphates    alcohol  \n",
       "0      0.560000   9.400000  \n",
       "1      0.680000   9.800000  \n",
       "2      0.650000   9.800000  \n",
       "3      0.580000   9.800000  \n",
       "4      0.560000   9.400000  \n",
       "...         ...        ...  \n",
       "4081   0.719419  11.569918  \n",
       "4082   0.772154  10.996139  \n",
       "4083   0.713169  12.519368  \n",
       "4084   0.723564  12.565524  \n",
       "4085   0.854692  12.959247  \n",
       "\n",
       "[4086 rows x 11 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proviamo a fare oversampling sul red wine portando il dataset\n",
    "# ad avere lo stesso numero di righe di quello bianco utilizzando SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Fit and apply SMOTE to the red wine dataset\n",
    "X_red_resampled, y_red_resampled = smote.fit_resample(X_red, y_red)\n",
    "print(f\"Resampled shape: {X_red_resampled.shape}\")\n",
    "\n",
    "X_red_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da12af",
   "metadata": {},
   "source": [
    "Ora che il dataset del vino rosso possiede un numero di righe confrontabili con quello del vino bianco, unisco il dataset resampled con quello bainco e rieffettuo il test sulla balanced accuracy dei 3 modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e9e74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "white    4898\n",
      "red      4086\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8979</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8980</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8981</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8982</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8983</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8984 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4              0.70         0.00             1.9      0.076   \n",
       "1               7.8              0.88         0.00             2.6      0.098   \n",
       "2               7.8              0.76         0.04             2.3      0.092   \n",
       "3              11.2              0.28         0.56             1.9      0.075   \n",
       "4               7.4              0.70         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "8979            6.2              0.21         0.29             1.6      0.039   \n",
       "8980            6.6              0.32         0.36             8.0      0.047   \n",
       "8981            6.5              0.24         0.19             1.2      0.041   \n",
       "8982            5.5              0.29         0.30             1.1      0.022   \n",
       "8983            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "8979                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "8980                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "8981                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "8982                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "8983                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  type  \n",
       "0         9.4        5     0  \n",
       "1         9.8        5     0  \n",
       "2         9.8        5     0  \n",
       "3         9.8        6     0  \n",
       "4         9.4        5     0  \n",
       "...       ...      ...   ...  \n",
       "8979     11.2        6     1  \n",
       "8980      9.6        5     1  \n",
       "8981      9.4        6     1  \n",
       "8982     12.8        7     1  \n",
       "8983     11.8        6     1  \n",
       "\n",
       "[8984 rows x 13 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "\n",
    "# Ricreo prima il dataset del vino rosso con X e y resampled\n",
    "red_wine_resampled = pd.concat([pd.DataFrame(X_red_resampled, columns=X_red.columns), \n",
    "                                 pd.Series(y_red_resampled, name='quality')], axis=1)\n",
    "\n",
    "wine_data_resampled = pd.concat([red_wine_resampled, white_wine_data], ignore_index=True)\n",
    "wine_data_resampled['type'] = [wine_type['red']] * len(red_wine_resampled) + [wine_type['white']] * len(white_wine_data)\n",
    "\n",
    "print(wine_data_resampled['type'].map({v: k for k, v in wine_type.items()}).value_counts())\n",
    "\n",
    "wine_data_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801c726",
   "metadata": {},
   "source": [
    "Nel nuovo dataset concatenato ci sono 4898 sample di vino bianco e 4086 sample di vino rosso, quindi direi che il dataset è abbastanza bilanciato, con un rapporto di 1.2:1 tra il vino bianco e il vino rosso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60c17b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.6452\n",
      "SVC Balanced Accuracy: 0.5700\n",
      "MLP Balanced Accuracy: 0.6009\n",
      "Decision Tree Accuracy: 0.6956\n",
      "SVC Accuracy: 0.6194\n",
      "MLP Accuracy: 0.6450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = wine_data_resampled.drop(columns=['quality'])\n",
    "y = wine_data_resampled['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6337f4b",
   "metadata": {},
   "source": [
    "# Risultati ottenuti su dataset artificiale bilanciato - oversampling\n",
    "| Modello | Balanced Accuracy - SMOTE | Accuracy - SMOTE |\n",
    "|---------|-------------------|-------------------|\n",
    "| Decision Tree | 0.6452 | 0.6956 |\n",
    "| SVC | 0.5700 | 0.6194 |\n",
    "| MLP | 0.6009 | 0.6450 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36fd83b",
   "metadata": {},
   "source": [
    "I risultati ottenuti sono molto migliori rispetto a quelli ottenuti con il dataset sbilanciato (w.r.t. numero di campioni di vino rosso e bainco), sia per quanto riguarda la balanced accuracy (che in questo caso è meno rilevante dal momento che il dataset è stato bilanciato) che l'accuracy. In particolare, il Decision Tree ha ottenuto la migliore performance, seguito dalla MLP e dalla SVC. Questo suggerisce che il Decision Tree potrebbe essere il modello più adatto per questo dataset bilanciato. Inoltre, questi risultati sono molto migliori di quelli ottenuti con i dataset separati, il che suggerisce che la fusione dei dataset potrebbe essere una buona strategia per migliorare le performance del modello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce687b",
   "metadata": {},
   "source": [
    "Non ci resta che effettuare l'undersampling del dataset dei vini bianchi, in modo da avere un dataset bilanciato con un numero di righe confrontabile con quello del vino rosso. In questo caso, il dataset dei vini bianchi verrà ridotto a 1600 righe, mentre il dataset dei vini rossi rimarrà invariato. Inoltre, dal momento che molti dei valori sono distribuiti attorno alla classe di `quality` pari a 5 o 6, ho deciso di effettuare l'undersampling cercando di mantere le proporzioni delle classi, in modo da non perdere informazioni importanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e61bf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.448755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.297468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.179665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         proportion\n",
       "quality            \n",
       "6          0.448755\n",
       "5          0.297468\n",
       "7          0.179665\n",
       "8          0.035729\n",
       "4          0.033279\n",
       "3          0.004083\n",
       "9          0.001021"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_target = len(red_wine_data)\n",
    "\n",
    "white_dist = white_wine_data['quality'].value_counts(normalize=True)\n",
    "\n",
    "pd.DataFrame(white_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f4d18",
   "metadata": {},
   "source": [
    "Come pronosticato, molti dei valori della qualità sono rappresentati dalla classe $5$ e $6$. Voglio mantere le proporzioni delle classi anche nel dataset undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3005408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.039</td>\n",
       "      <td>38.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.99300</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.51</td>\n",
       "      <td>9.7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.024</td>\n",
       "      <td>20.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.99208</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.119</td>\n",
       "      <td>33.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99210</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>20.30</td>\n",
       "      <td>0.037</td>\n",
       "      <td>45.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.99727</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.046</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.99775</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.43</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>8.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>15.55</td>\n",
       "      <td>0.057</td>\n",
       "      <td>35.5</td>\n",
       "      <td>366.5</td>\n",
       "      <td>1.00010</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0.63</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>10.3</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.037</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.99390</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.28</td>\n",
       "      <td>9.6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.047</td>\n",
       "      <td>146.5</td>\n",
       "      <td>307.5</td>\n",
       "      <td>0.99240</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.021</td>\n",
       "      <td>24.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.98965</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.61</td>\n",
       "      <td>12.4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.032</td>\n",
       "      <td>31.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99030</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>12.9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               6.2              0.15         0.46            1.60      0.039   \n",
       "1               6.9              0.31         0.32            1.20      0.024   \n",
       "2               6.0              0.28         0.34            1.60      0.119   \n",
       "3               6.8              0.30         0.26           20.30      0.037   \n",
       "4               6.2              0.30         0.26           13.40      0.046   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1595            8.6              0.55         0.35           15.55      0.057   \n",
       "1596           10.3              0.17         0.47            1.40      0.037   \n",
       "1597            7.1              0.49         0.22            2.00      0.047   \n",
       "1598            6.6              0.36         0.29            1.60      0.021   \n",
       "1599            7.1              0.26         0.49            2.20      0.032   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    38.0                 123.0  0.99300  3.38       0.51   \n",
       "1                    20.0                 166.0  0.99208  3.05       0.54   \n",
       "2                    33.0                 104.0  0.99210  3.19       0.38   \n",
       "3                    45.0                 150.0  0.99727  3.04       0.38   \n",
       "4                    57.0                 206.0  0.99775  3.17       0.43   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1595                 35.5                 366.5  1.00010  3.04       0.63   \n",
       "1596                  5.0                  33.0  0.99390  2.89       0.28   \n",
       "1597                146.5                 307.5  0.99240  3.24       0.37   \n",
       "1598                 24.0                  85.0  0.98965  3.41       0.61   \n",
       "1599                 31.0                 113.0  0.99030  3.37       0.42   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.7        6  \n",
       "1         9.8        6  \n",
       "2        10.2        6  \n",
       "3        12.3        6  \n",
       "4         9.5        6  \n",
       "...       ...      ...  \n",
       "1595     11.0        3  \n",
       "1596      9.6        3  \n",
       "1597     11.0        3  \n",
       "1598     12.4        9  \n",
       "1599     12.9        9  \n",
       "\n",
       "[1600 rows x 12 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ottengo il numero di campioni desiderati per ogni classe\n",
    "n_per_class = (white_dist * n_target).round().astype(int)\n",
    "\n",
    "white_wine_undersampled = pd.DataFrame()\n",
    "for label, n_samples in n_per_class.items():\n",
    "    subset = white_wine_data[white_wine_data['quality'] == label]\n",
    "    sampled_subset = subset.sample(n=n_samples, random_state=42)\n",
    "    white_wine_undersampled = pd.concat([white_wine_undersampled, sampled_subset], ignore_index=True)\n",
    "\n",
    "white_wine_undersampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8bf4a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Balanced Accuracy: 0.3704\n",
      "SVC Balanced Accuracy: 0.2695\n",
      "MLP Balanced Accuracy: 0.2803\n",
      "Decision Tree Accuracy: 0.5391\n",
      "SVC Accuracy: 0.5875\n",
      "MLP Accuracy: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\micha\\Desktop\\Materie\\Rossi-Manno\\rossi-manno\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2524: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now I create the final dataset with the undersampled white wine\n",
    "wine_data_undersampled = pd.concat([red_wine_data, white_wine_undersampled], ignore_index=True)\n",
    "wine_data_undersampled['type'] = [wine_type['red']] * len(red_wine_data) + [wine_type['white']] * len(white_wine_undersampled)\n",
    "\n",
    "X = wine_data_undersampled.drop(columns=['quality'])\n",
    "y = wine_data_undersampled['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "# Create and train the classifiers\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "svc_classifier = SVC(random_state=42)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifiers\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "svc_predictions = svc_classifier.predict(X_test)\n",
    "mlp_predictions = mlp_classifier.predict(X_test)\n",
    "\n",
    "dt_balanced_accuracy = balanced_accuracy_score(y_test, dt_predictions)\n",
    "svc_balanced_accuracy = balanced_accuracy_score(y_test, svc_predictions)\n",
    "mlp_balanced_accuracy = balanced_accuracy_score(y_test, mlp_predictions)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Decision Tree Balanced Accuracy: {dt_balanced_accuracy:.4f}\")\n",
    "print(f\"SVC Balanced Accuracy: {svc_balanced_accuracy:.4f}\")\n",
    "print(f\"MLP Balanced Accuracy: {mlp_balanced_accuracy:.4f}\")\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.4f}\")\n",
    "print(f\"SVC Accuracy: {svc_accuracy:.4f}\")\n",
    "print(f\"MLP Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff44bc4",
   "metadata": {},
   "source": [
    "# Risultati ottenuti su dataset artificiale bilanciato - undersampling\n",
    "| Modello | Balanced Accuracy | Accuracy |\n",
    "|---------|-------------------|-------------------|\n",
    "| Decision Tree | 0.3704 | 0.5391 |\n",
    "| SVC | 0.2695 | 0.5875 |\n",
    "| MLP | 0.2803 | 0.5609 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5c2e7",
   "metadata": {},
   "source": [
    "Le performance della balanced accuracy e dell'accuracy sono molto basse rispetto a quelle ottenute con l'oversampling, il che suggerisce che l'undersampling potrebbe non essere la strategia migliore per questo dataset. In particolare, il Decision Tree ha ottenuto la migliore performance usando la balanced accuracy mentre l'SVC ha ottenuto la migliore performance usando l'accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91008d",
   "metadata": {},
   "source": [
    "Ecco una tabella unica che confronta i risultati ottenuti con i diversi metodi di bilanciamento e modelli di classificazione:\n",
    "\n",
    "| Modello        | Balanced Accuracy (Undersampling) | Accuracy (Undersampling) | Balanced Accuracy (Oversampling/SMOTE) | Accuracy (Oversampling/SMOTE) |\n",
    "| -------------- | --------------------------------- | ------------------------ | -------------------------------------- | ----------------------------- |\n",
    "| Decision Tree  | 0.3704                            | 0.5391                   | 0.6452                                 | 0.6956                        |\n",
    "| SVC            | 0.2695                            | 0.5875                   | 0.5700                                 | 0.6194                        |\n",
    "| MLP            | 0.2803                            | 0.5609                   | 0.6009                                 | 0.6450                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bbe83",
   "metadata": {},
   "source": [
    "L'oversampling con SMOTE ha prodotto risultati significativamente migliori rispetto all'undersampling, sia in termini di balanced accuracy che di accuracy. In particolare, il Decision Tree ha ottenuto le migliori performance in entrambi i casi, seguito dalla MLP e dalla SVC. Questo suggerisce che l'oversampling potrebbe essere la strategia migliore per questo dataset, in quanto consente di mantenere un numero maggiore di campioni e quindi di preservare più informazioni utili per la classificazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d39e2",
   "metadata": {},
   "source": [
    "# Conclusioni\n",
    "A questo punto, siamo rimasti con due scelte:  \n",
    "- Oversampling con SMOTE per il dataset del vino rosso con successiva fusione con il dataset del vino bianco, ottenendo un dataset bilanciato con circa 9000 righe.  \n",
    "- Allenare i modelli separatamente sui due dataset, ottenendo un average balanced accuracy e un average accuracy per i due dataset.\n",
    "\n",
    "### Riepilogo risultati\n",
    "\n",
    "| Modello        | Average Balanced Accuracy | Average Accuracy | Balanced Accuracy - SMOTE | Accuracy - SMOTE |\n",
    "| -------------- | :----------------------: | :--------------: | :----------------------: | :--------------: |\n",
    "| Decision Tree  |         0.3618           |     0.5864       |         0.6452           |     0.6956       |\n",
    "| SVC            |         0.2745           |     0.5822       |         0.5700           |     0.6194       |\n",
    "| MLP            |         0.3356           |     0.5935       |         0.6009           |     0.6450       |\n",
    "\n",
    "In conclusione, l'oversampling con SMOTE ha prodotto risultati migliori rispetto a tutte le alternative vagliate. Tuttavia, è importante notare che i risultati ottenuti sono ancora lontani dall'essere ottimali e potrebbero essere migliorati ulteriormente con un'adeguata selezione delle feature, un tuning degli iperparametri e l'utilizzo di tecniche di preprocessing più avanzate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rossi-manno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
